# -*- coding: utf-8 -*-
"""stress_model1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x62Q96jGo-XolS4uFTR_kGz88hYiOYty
"""



import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.inspection import permutation_importance

# Step 1: Load the dataset with the correct delimiter
data = pd.read_csv('data_stress-2.csv', sep=';')

# Step 2: Clean column names by stripping whitespace
data.columns = data.columns.str.strip()

# Step 3: Replace commas with dots and convert to numeric
for column in data.columns[:-1]:
    data[column] = data[column].str.replace(',', '.').astype(float)

# Step 4: Ensure the target variable is numeric
data['Stress Levels'] = data['Stress Levels'].astype(int)

# Step 5: Check for missing values
print("Missing values in the dataset:\n", data.isnull().sum())

# Step 6: Handling Missing Values
# Create binary features indicating missing values
for column in ['body temperature', 'limb movement', 'blood oxygen',
               'eye movement', 'hours of sleep', 'heart rate']:
    data[f'{column}_missing'] = data[column].isnull().astype(int)

# Impute missing values with the mean
imputer = SimpleImputer(strategy='mean')
data.iloc[:, :-1] = imputer.fit_transform(data.iloc[:, :-1])  # Imputing only feature columns

# Step 7: Select features and target labels
X = data[['snoring range', 'respiration rate', 'body temperature', 'limb movement',
           'blood oxygen', 'eye movement', 'hours of sleep', 'heart rate'] +
           [f'{column}_missing' for column in ['body temperature', 'limb movement',
                                                'blood oxygen', 'eye movement',
                                                'hours of sleep', 'heart rate']]]
y = data['Stress Levels']

# Step 8: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 9: Normalize the feature data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 10: Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

# Step 11: Create and train the MLPClassifier model
model = MLPClassifier(max_iter=1000, random_state=42, early_stopping=True, validation_fraction=0.2)

# Step 12: Define hyperparameters to tune
param_grid = {
    'hidden_layer_sizes': [(10, 10), (20, 10), (20, 20)],
    'activation': ['tanh', 'relu'],
    'solver': ['sgd', 'adam'],
    'alpha': [0.0001, 0.001, 0.01],
    'learning_rate': ['constant', 'adaptive'],
}

# Step 13: Perform GridSearchCV to find the best model
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(X_train_balanced, y_train_balanced)

# Step 14: Get the best model from GridSearchCV
best_model = grid_search.best_estimator_
print("Best Hyperparameters:", grid_search.best_params_)

# Step 15: Evaluate the best model using cross-validation
cv_scores = cross_val_score(best_model, X_train_balanced, y_train_balanced, cv=5)
print(f"Cross-Validation Scores: {cv_scores}")
print(f"Average Cross-Validation Score: {np.mean(cv_scores)}")

# Step 16: Train the best model on the balanced training data
best_model.fit(X_train_balanced, y_train_balanced)

# Step 17: Make predictions on the test data
y_pred = best_model.predict(X_test)

# Step 18: Evaluate the model with a confusion matrix and classification report
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Step 19: Visualize the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

# Step 20: Plot the training loss curve
plt.plot(best_model.loss_curve_)
plt.title('Model Training Loss Curve')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.show()

# Step 21: Feature Importance using Permutation Importance
result = permutation_importance(best_model, X_test, y_test, n_repeats=30, random_state=42)
sorted_idx = result.importances_mean.argsort()
# Step 22: Plot Feature Importance
plt.figure(figsize=(10, 6))
plt.barh(range(len(sorted_idx)), result.importances_mean[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), np.array(X.columns)[sorted_idx])
plt.title('Feature Importance')
plt.xlabel('Permutation Importance')
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.inspection import permutation_importance

# Step 1: Load the dataset with the correct delimiter
data = pd.read_csv('data_stress-2.csv', sep=';')

# Step 2: Clean column names by stripping whitespace
data.columns = data.columns.str.strip()

# Step 3: Replace commas with dots and convert to numeric
for column in data.columns[:-1]:
    data[column] = data[column].str.replace(',', '.').astype(float)

# Step 4: Ensure the target variable is numeric
data['Stress Levels'] = data['Stress Levels'].astype(int)

# Step 5: Check for missing values
print("Missing values in the dataset:\n", data.isnull().sum())

# Step 6: Handling Missing Values
# Create binary features indicating missing values
for column in ['body temperature', 'limb movement', 'blood oxygen',
               'eye movement', 'hours of sleep', 'heart rate']:
    data[f'{column}_missing'] = data[column].isnull().astype(int)

# Impute missing values with the mean
imputer = SimpleImputer(strategy='mean')
data.iloc[:, :-1] = imputer.fit_transform(data.iloc[:, :-1])  # Imputing only feature columns

# Step 7: Select features and target labels
X = data[['snoring range', 'respiration rate', 'body temperature', 'limb movement',
           'blood oxygen', 'eye movement', 'hours of sleep', 'heart rate'] +
           [f'{column}_missing' for column in ['body temperature', 'limb movement',
                                                'blood oxygen', 'eye movement',
                                                'hours of sleep', 'heart rate']]]
y = data['Stress Levels']

# Step 8: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 9: Normalize the feature data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 10: Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

# Step 11: Create and train the MLPClassifier model
model = MLPClassifier(max_iter=1000, random_state=42, early_stopping=True, validation_fraction=0.2)

# Step 12: Define hyperparameters to tune
param_grid = {
    'hidden_layer_sizes': [(10, 10), (20, 10), (20, 20)],
    'activation': ['tanh', 'relu'],
    'solver': ['sgd', 'adam'],
    'alpha': [0.0001, 0.001, 0.01],
    'learning_rate': ['constant', 'adaptive'],
}

# Step 13: Perform GridSearchCV to find the best model
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(X_train_balanced, y_train_balanced)

# Step 14: Get the best model from GridSearchCV
best_model = grid_search.best_estimator_
print("Best Hyperparameters:", grid_search.best_params_)

# Step 15: Evaluate the best model using cross-validation
cv_scores = cross_val_score(best_model, X_train_balanced, y_train_balanced, cv=5)
print(f"Cross-Validation Scores: {cv_scores}")
print(f"Average Cross-Validation Score: {np.mean(cv_scores)}")

# Step 16: Train the best model on the balanced training data
best_model.fit(X_train_balanced, y_train_balanced)

# Step 17: Make predictions on the test data
y_pred = best_model.predict(X_test)

# Step 18: Evaluate the model with a confusion matrix and classification report
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Step 19: Visualize the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

# Step 20: Plot the training loss curve
plt.plot(best_model.loss_curve_)
plt.title('Model Training Loss Curve')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.show()

# Step 21: Feature Importance using Permutation Importance
result = permutation_importance(best_model, X_test, y_test, n_repeats=30, random_state=42)
sorted_idx = result.importances_mean.argsort()
# Step 22: Plot Feature Importance
plt.figure(figsize=(10, 6))
plt.barh(range(len(sorted_idx)), result.importances_mean[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), np.array(X.columns)[sorted_idx])
plt.title('Feature Importance')
plt.xlabel('Permutation Importance')
plt.show()

# Step 23: Output Predictions for Stress Levels
# Predict stress levels for test data
y_pred_stress_levels = best_model.predict(X_test)
print("Predicted Stress Levels:", y_pred_stress_levels)

# Save predictions to a DataFrame with test data for review
predictions_df = pd.DataFrame({'Actual Stress Level': y_test, 'Predicted Stress Level': y_pred_stress_levels})
predictions_df.to_csv('predicted_stress_levels.csv', index=False)

# Step 24: Define function for real-time predictions on new data
def predict_stress(input_data):
    # Preprocess input data
    input_data = scaler.transform(input_data)  # Apply same scaler used in training
    # Predict stress level
    return best_model.predict(input_data)

# Example usage with new data
new_data = np.array([[4.5, 18.0, 36.6, 0.0, 98.1, 1.0, 7.0, 75.0, 0, 0, 0, 0, 0, 0]])  # Replace with real values
print("Predicted Stress Level for New Data:", predict_stress(new_data))

import joblib

# Save the model and scaler
joblib.dump(best_model, 'stress_level_model.pkl')
joblib.dump(scaler, 'scaler.pkl')

import numpy as np
import joblib

# Load the saved model and scaler
model = joblib.load('stress_level_model.pkl')
scaler = joblib.load('scaler.pkl')

# Define a function for predicting stress levels
def predict_stress(new_data):
    """
    Predicts stress levels based on input data.

    Parameters:
    new_data (ndarray): A NumPy array of shape (n_samples, n_features) containing new data.
                        Example data should be structured similarly to the original feature set.

    Returns:
    ndarray: Predicted stress levels for each input sample.
    """
    # Apply scaler to normalize new data
    new_data_scaled = scaler.transform(new_data)

    # Predict stress levels
    return model.predict(new_data_scaled)

# Example usage with sample data
# Replace these values with actual inputs for real predictions
sample_data = np.array([[4.5, 18.0, 36.6, 0.0, 98.1, 1.0, 7.0, 75.0, 0, 0, 0, 0, 0, 0]])
predicted_stress = predict_stress(sample_data)
print("Predicted Stress Level for Sample Data:", predicted_stress)

# Import necessary libraries
import numpy as np
import pandas as pd
import joblib

# Load the saved model and scaler
model = joblib.load('stress_level_model.pkl')
scaler = joblib.load('scaler.pkl')

# Define the column names (must match the original training features)
column_names = ['snoring range', 'respiration rate', 'body temperature', 'limb movement',
                'blood oxygen', 'eye movement', 'hours of sleep', 'heart rate',
                'body temperature_missing', 'limb movement_missing', 'blood oxygen_missing',
                'eye movement_missing', 'hours of sleep_missing', 'heart rate_missing']

# Define the prediction function
def predict_stress(new_data):
    """
    Predicts stress levels based on new input data.

    Parameters:
    new_data (ndarray): A NumPy array of shape (n_samples, n_features) containing new data.
                        Each row should contain values for each feature in the order specified
                        by `column_names`.

    Returns:
    ndarray: Predicted stress levels for each sample.
    """
    # Convert new_data to DataFrame to match original feature names
    new_data_df = pd.DataFrame(new_data, columns=column_names)

    # Scale the new data
    new_data_scaled = scaler.transform(new_data_df)

    # Predict stress levels
    predictions = model.predict(new_data_scaled)
    return predictions

# Example usage with sample data
sample_data = np.array([[4.5, 18.0, 36.6, 0.0, 98.1, 1.0, 7.0, 75.0, 0, 0, 0, 0, 0, 0]])
predicted_stress_levels = predict_stress(sample_data)

# Display the prediction result
print("Predicted Stress Level:", predicted_stress_levels)



test_samples = np.array([
    [4.5, 18.0, 36.6, 0.0, 98.1, 1.0, 7.0, 75.0, 0, 0, 0, 0, 0, 0],  # Low-stress case
    [6.0, 20.5, 38.5, 1.0, 92.5, 1.5, 5.0, 85.0, 0, 1, 0, 1, 0, 1],  # Moderate stress indicators
    [7.5, 24.0, 39.0, 2.0, 90.0, 2.5, 4.0, 95.0, 1, 1, 1, 1, 1, 1]   # High-stress case
])

for i, sample in enumerate(test_samples, 1):
    stress_prediction = predict_stress(sample.reshape(1, -1))
    print(f"Predicted Stress Level for Sample {i}: {stress_prediction}")

import pandas as pd

# Load the dataset
data = pd.read_csv('data_stress-2.csv', sep=';')

# Step 1: Clean column names by stripping whitespace
data.columns = data.columns.str.strip()

# Step 2: Replace commas with dots and convert to numeric
for column in data.columns[:-1]:  # Exclude the target variable ('Stress Levels') if it's the last column
    data[column] = data[column].str.replace(',', '.').astype(float)

# Step 3: Convert target variable 'Stress Levels' to numeric
data['Stress Levels'] = data['Stress Levels'].astype(int)

# Step 4: Calculate average stress level
average_stress_level = data['Stress Levels'].mean()
print(f"Average Stress Level: {average_stress_level}")

# Step 5: Calculate the average of each feature and general statistics
feature_means = data.mean()
print("\nFeature Averages:")
print(feature_means)

# Step 6: Summary statistics for more insight (optional)
summary_stats = data.describe()
print("\nSummary Statistics:")
print(summary_stats)



import pandas as pd

# Load the dataset
data = pd.read_csv('data_stress-2.csv', sep=';')

# Clean column names
data.columns = data.columns.str.strip()

# Ensure the 'Stress Levels' column is of integer type
data['Stress Levels'] = data['Stress Levels'].astype(int)

# Count occurrences of each stress level
stress_level_counts = data['Stress Levels'].value_counts()

# Display the counts for each stress level
print("Number of occurrences for each stress level:")
print(stress_level_counts)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Load and clean the dataset
data = pd.read_csv('data_stress-2.csv', sep=';')
data.columns = data.columns.str.strip()  # Clean column names

# Step 2: Replace commas with dots and convert to numeric for all columns except 'Stress Levels'
for column in data.columns[:-1]:  # Exclude 'Stress Levels' column
    data[column] = data[column].str.replace(',', '.').astype(float)

# Ensure the 'Stress Levels' column is of integer type
data['Stress Levels'] = data['Stress Levels'].astype(int)

# Step 3: Handle Missing Values
# Use SimpleImputer to fill missing values with the mean of each column
imputer = SimpleImputer(strategy='mean')
data.iloc[:, :-1] = imputer.fit_transform(data.iloc[:, :-1])  # Impute only the feature columns

# Step 4: Select features and target labels
X = data[['snoring range', 'respiration rate', 'body temperature', 'limb movement',
           'blood oxygen', 'eye movement', 'hours of sleep', 'heart rate']]
y = data['Stress Levels']

# Step 5: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 6: Normalize the feature data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 7: Create and train the MLPClassifier model
model = MLPClassifier(max_iter=1000, random_state=42, early_stopping=True, validation_fraction=0.2)
model.fit(X_train, y_train)

# Step 8: Make predictions on the test data
y_pred = model.predict(X_test)

# Step 9: Evaluate the model with confusion matrix and classification report
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Step 10: Visualize the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

# Count the occurrences of each stress level
stress_counts = data['Stress Levels'].value_counts().sort_index()

# Display the counts of each stress level
print("Number of occurrences for each stress level:")
print(stress_counts)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# Load the dataset
data = pd.read_csv('data_stress-2.csv', sep=';')

# Step 1: Clean column names by stripping whitespace
data.columns = data.columns.str.strip()

# Step 2: Replace commas with dots and convert to numeric (for all columns except target)
for column in data.columns[:-1]:  # Exclude the target variable ('Stress Levels') if it's the last column
    data[column] = data[column].str.replace(',', '.').astype(float)

# Step 3: Convert target variable 'Stress Levels' to numeric
data['Stress Levels'] = data['Stress Levels'].astype(int)

# Step 4: Calculate average stress level
average_stress_level = data['Stress Levels'].mean()
print(f"Average Stress Level: {average_stress_level}")

# Step 5: Calculate the average of each feature and general statistics
feature_means = data.mean()
print("\nFeature Averages:")
print(feature_means)

# Step 6: Summary statistics for more insight (optional)
summary_stats = data.describe()
print("\nSummary Statistics:")
print(summary_stats)

# Step 7: Feature selection (excluding 'Stress Levels')
X = data.drop('Stress Levels', axis=1)  # Features
y = data['Stress Levels']  # Target variable (Stress Levels)

# Step 8: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 9: Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 10: Train a Random Forest classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# Step 11: Make predictions
y_pred = model.predict(X_test_scaled)

# Step 12: Evaluate the model
print("\nAccuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Optional: Feature importance (to see which features are most influential in predicting stress levels)
feature_importance = model.feature_importances_
print("\nFeature Importance:")
for feature, importance in zip(X.columns, feature_importance):
    print(f"{feature}: {importance:.4f}")

import pandas as pd
import numpy as np

# Set the random seed for reproducibility
np.random.seed(42)

# Define the number of samples
num_samples = 100

# Generate random data for 7 features
data = {
    'Feature_1': np.random.uniform(0, 100, num_samples),
    'Feature_2': np.random.uniform(0, 50, num_samples),
    'Feature_3': np.random.uniform(0, 100, num_samples),
    'Feature_4': np.random.uniform(0, 25, num_samples),
    'Feature_5': np.random.uniform(0, 10, num_samples),
    'Feature_6': np.random.uniform(50, 200, num_samples),
    'Feature_7': np.random.uniform(40, 100, num_samples),
    'Stress Levels': np.random.choice([0, 1, 2], num_samples)  # 0 = Low, 1 = Medium, 2 = High
}

# Create a DataFrame from the generated data
df = pd.DataFrame(data)

# Save the DataFrame to a CSV file
df.to_csv('random_stress_data.csv', index=False)

# Print the first few rows to verify the data
print(df.head())

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.neural_network import MLPClassifier

# Step 1: Load and preprocess the original data
data = pd.read_csv('data_stress-2.csv', sep=';')
data.columns = data.columns.str.strip()  # Clean column names

# Replace commas with dots and convert to float for all columns except 'Stress Levels'
for column in data.columns[:-1]:  # Exclude 'Stress Levels' column
    data[column] = data[column].str.replace(',', '.').astype(float)

# Ensure the 'Stress Levels' column is of integer type
data['Stress Levels'] = data['Stress Levels'].astype(int)

# Handle missing values with SimpleImputer (mean strategy)
imputer = SimpleImputer(strategy='mean')
data.iloc[:, :-1] = imputer.fit_transform(data.iloc[:, :-1])  # Impute only the feature columns

# Step 2: Prepare features and target labels
X = data[['snoring range', 'respiration rate', 'body temperature', 'limb movement',
           'blood oxygen', 'eye movement', 'hours of sleep', 'heart rate']]
y = data['Stress Levels']

# Step 3: Normalize the features using StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 4: Train the MLPClassifier model
model = MLPClassifier(max_iter=1000, random_state=42, early_stopping=True, validation_fraction=0.2)
model.fit(X_scaled, y)

# Step 5: Generate random test data (with the same number of features)
random_data = np.random.rand(1, 8)  # 1 sample, 8 features

# Step 6: Convert random data into a DataFrame with feature names
random_data_df = pd.DataFrame(random_data, columns=X.columns)

# Step 7: Normalize the random data using the fitted scaler
random_data_scaled = scaler.transform(random_data_df)

# Step 8: Make predictions
prediction = model.predict(random_data_scaled)

# Step 9: Display the prediction (Stress Level)
print(f"Predicted Stress Level for the random input: {prediction[0]}")

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Step 1: Load the trained model (assuming it's already trained)
# model = joblib.load('your_trained_model.pkl')  # Load your trained model
# Here we're assuming you already have a trained model and scaler

# Fit a new scaler on your training data (you can use your original training data here)
# For illustration, I'll assume 'X_train' is available (the training data features)

# Sample training data generation (replace this with your actual training data)
# You should already have X_train from your training phase
X_train = np.random.rand(100, 8)  # Example training data (100 samples, 8 features)

# Fit the scaler on the training data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Step 2: Generate random test cases
random_data = np.random.rand(10, 8)  # 10 random test cases, 8 features

# Define feature ranges as before
feature_ranges = {
    'snoring range': (0, 10),
    'respiration rate': (10, 40),
    'body temperature': (36, 39),
    'limb movement': (0, 100),
    'blood oxygen': (90, 100),
    'eye movement': (0, 10),
    'hours of sleep': (3, 10),
    'heart rate': (50, 100),
}

# Scale the random data to match the feature ranges
for i, (feature, (low, high)) in enumerate(feature_ranges.items()):
    random_data[:, i] = random_data[:, i] * (high - low) + low

# Step 3: Normalize the test data using the fitted scaler
random_data_scaled = scaler.transform(random_data)  # Use the fitted scaler here

# Step 4: Make predictions using the trained model (assuming 'model' is already trained)
# predicted_stress_levels = model.predict(random_data_scaled)  # Predict stress levels

# Since you are not loading a real model here, I'll assume the model is available
# For demonstration purposes, let's simulate predictions:
predicted_stress_levels = np.random.randint(0, 5, 10)  # Simulate some random predictions (stress levels between 0 and 4)

# Step 5: Print out the predicted stress levels for each random test case
for i, stress_level in enumerate(predicted_stress_levels, 1):
    print(f"Test Case {i}: Predicted Stress Level for the random input: {stress_level}")

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Load and clean the dataset
data = pd.read_csv('data_stress-2.csv', sep=';')
data.columns = data.columns.str.strip()  # Clean column names

# Step 2: Replace commas with dots and convert to numeric for all columns except 'Stress Levels'
for column in data.columns[:-1]:  # Exclude 'Stress Levels' column
    data[column] = data[column].str.replace(',', '.').astype(float)

# Ensure the 'Stress Levels' column is of integer type
data['Stress Levels'] = data['Stress Levels'].astype(int)

# Step 3: Handle Missing Values
imputer = SimpleImputer(strategy='mean')
data.iloc[:, :-1] = imputer.fit_transform(data.iloc[:, :-1])

# Step 4: Select features and target labels
X = data[['snoring range', 'respiration rate', 'body temperature', 'limb movement',
          'blood oxygen', 'eye movement', 'hours of sleep', 'heart rate']]
y = data['Stress Levels']

# Step 5: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 6: Normalize the feature data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 7: Apply SMOTE to balance classes in the training set
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Step 8: Hyperparameter tuning for MLPClassifier
param_grid = {
    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],
    'alpha': [0.0001, 0.001, 0.01],
    'learning_rate_init': [0.001, 0.01, 0.1]
}
grid_search = GridSearchCV(MLPClassifier(max_iter=1000, random_state=42, early_stopping=True, validation_fraction=0.2),
                           param_grid, cv=5)
grid_search.fit(X_train_resampled, y_train_resampled)
best_mlp_model = grid_search.best_estimator_

# Step 9: Train the best MLP model on the resampled training data
best_mlp_model.fit(X_train_resampled, y_train_resampled)

# Step 10: Make predictions on the test set with the MLP model
y_pred_mlp = best_mlp_model.predict(X_test)

# Step 11: Alternative Model - RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_resampled, y_train_resampled)
y_pred_rf = rf_model.predict(X_test)

# Step 12: Evaluate both models
print("MLP Classifier Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_mlp):.2f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred_mlp))

print("Random Forest Classifier Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf):.2f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred_rf))

# Step 13: Visualize confusion matrices for both models
plt.figure(figsize=(12, 5))

# Confusion Matrix for MLP
plt.subplot(1, 2, 1)
sns.heatmap(confusion_matrix(y_test, y_pred_mlp), annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title('Confusion Matrix - MLP Classifier')
plt.xlabel('Predicted')
plt.ylabel('Actual')

# Confusion Matrix for Random Forest
plt.subplot(1, 2, 2)
sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title('Confusion Matrix - Random Forest Classifier')
plt.xlabel('Predicted')
plt.ylabel('Actual')

plt.tight_layout()
plt.show()

import numpy as np

# Generate 10 random test cases based on expected feature ranges
test_cases = [
    {
        'snoring range': np.random.uniform(0, 100),
        'respiration rate': np.random.uniform(10, 30),
        'body temperature': np.random.uniform(36.5, 39.0),
        'limb movement': np.random.uniform(0, 10),
        'blood oxygen': np.random.uniform(80, 100),
        'eye movement': np.random.uniform(0, 10),
        'hours of sleep': np.random.uniform(3, 12),
        'heart rate': np.random.uniform(60, 120)
    }
    for _ in range(10)
]

# Convert test cases into DataFrame for easy scaling
test_df = pd.DataFrame(test_cases)

# Normalize the test cases using the same scaler as the training data
test_df_scaled = scaler.transform(test_df)

# Make predictions using the trained MLP model
predicted_stress_levels = best_mlp_model.predict(test_df_scaled)

# Output predictions for each test case
for i, (test_case, predicted_level) in enumerate(zip(test_cases, predicted_stress_levels), start=1):
    print(f"Test Case {i}:")
    print("Input Features:")
    for feature, value in test_case.items():
        print(f"  {feature}: {value:.2f}")
    print(f"Predicted Stress Level: {predicted_level}")
    print("-" * 30)
    
    
import os

# Save the best model to a file
joblib.dump(best_model, 'test.pkl')

# Check if the file has been created
if os.path.exists('test.pkl'):
    print("Model file 'best_stress_model.pkl' created successfully.")
else:
    print("Model file 'best_stress_model.pkl' was not created.")


